{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICAIF 2024 금융-RAG 챌린지 기본 예제\n",
    "\n",
    "이 노트북은 **ICAIF 2024 금융-RAG 챌린지**를 위한 **기본 예제**입니다. 이 챌린지의 목표는 금융 데이터를 위한 **Retrieval-Augmented Generation (RAG)** 시스템을 만드는 것입니다. 참가자는 대규모 코퍼스에서 관련 문서를 검색하고 사용자 Query에 대한 정확하고 상황에 맞는 응답을 제공하는 시스템을 개발해야 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 시스템 구성 요소\n",
    "\n",
    "기본 예제의 시스템은 두 가지 주요 구성 요소로 나뉩니다:\n",
    "\n",
    "1. **검색**: 사용자 쿼리를 기반으로 대규모 금융 문서 코퍼스에서 관련 문서를 검색합니다.\n",
    "2. **재정렬**: 검색된 문서의 순위를 다시 매겨 가장 관련성 높은 정보가 우선되도록 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 모델 개요\n",
    "\n",
    "이 베이스라인 노트북은 `SentenceTransformer`와 `CrossEncoder` 모델을 조합하여 다음 작업을 수행합니다:\n",
    "\n",
    "- **검색 모델**은 쿼리와 문서를 임베딩으로 인코딩하는 역할을 담당합니다.\n",
    "- **재정렬 모델**은 검색된 문서의 관련성을 평가하고 순서를 조정합니다.\n",
    "\n",
    "이 예시에서는 **FinDER**라는 FinanceRAG 프로젝트의 7개 과제 중 하나를 사용합니다. 검색 모델로는 `intfloat/e5-large-v2`가 사용되며, 재정렬은 `cross-encoder/ms-marco-MiniLM-L-12-v2`를 통해 수행됩니다. 두 모델 모두 `sentence_transformers` 라이브러리에서 지원하는 다른 모델로 대체하여 성능을 실험해볼 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 목표\n",
    "\n",
    "이 노트북의 목표는 참가자들이 챌린지를 위한 보다 **고급 솔루션**을 구축할 수 있는 **탄탄한 기반**을 제공하는 것입니다. 과제, 검색 모델 및 재정렬 모델을 필요에 따라 자유롭게 개발하세요!\n",
    "\n",
    "---\n",
    "\n",
    "## Repository Setup and Environment Configuration\n",
    "\n",
    "GitHub 리포지토리 확인 [here](https://github.com/linq-rag/FinanceRAG).\n",
    "\n",
    "아래와 같이 Github repository를 Clone하기:\n",
    "\n",
    "### 1. Clone the repository:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/linq-rag/FinanceRAG.git\n",
    "cd FinanceRAG\n",
    "```\n",
    "\n",
    "### 2. Set up the Python environment:\n",
    "\n",
    "#### If using `venv` (Python 3.11 or higher required):\n",
    "\n",
    "```bash\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate  # On Windows use .venv\\Scripts\u0007ctivate\n",
    "pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "#### If using `conda`:\n",
    "\n",
    "```bash\n",
    "conda create -n financerag python=3.11\n",
    "conda activate financerag\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "준비가 완료되었습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T14:50:33.702172Z",
     "iopub.status.busy": "2024-10-04T14:50:33.701317Z",
     "iopub.status.idle": "2024-10-04T14:51:15.803357Z",
     "shell.execute_reply": "2024-10-04T14:51:15.802271Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# --------------------------------------\n",
    "# Import required libraries for document retrieval, reranking, and logging setup.\n",
    "from sentence_transformers import CrossEncoder\n",
    "import logging\n",
    "\n",
    "from financerag.rerank import CrossEncoderReranker\n",
    "from financerag.retrieval import DenseRetrieval, SentenceTransformerEncoder\n",
    "from financerag.tasks import FinDER\n",
    "import torch\n",
    "\n",
    "# Setup basic logging configuration to show info level messages.\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T14:51:15.808338Z",
     "iopub.status.busy": "2024-10-04T14:51:15.807849Z",
     "iopub.status.idle": "2024-10-04T14:51:23.956758Z",
     "shell.execute_reply": "2024-10-04T14:51:23.955721Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:financerag.common.loader:A Hugging Face repository is provided. This will override the data_folder, prefix, and *_file arguments.\n",
      "INFO:financerag.common.loader:Loading Corpus...\n",
      "INFO:financerag.common.loader:Loaded 13867 Documents.\n",
      "INFO:financerag.common.loader:Corpus Example: {'id': 'ADBE20230004', 'title': 'ADBE OVERVIEW', 'text': 'Adobe is a global technology company with a mission to change the world through personalized digital experiences. For over four decades, Adobe’s innovations have transformed how individuals, teams, businesses, enterprises, institutions, and governments engage and interact across all types of media. Our products, services and solutions are used around the world to imagine, create, manage, deliver, measure, optimize and engage with content across surfaces and fuel digital experiences. We have a diverse user base that includes consumers, communicators, creative professionals, developers, students, small and medium businesses and enterprises. We are also empowering creators by putting the power of artificial intelligence (“AI”) in their hands, and doing so in ways we believe are responsible. Our products and services help unleash creativity, accelerate document productivity and power businesses in a digital world.'}\n",
      "INFO:financerag.common.loader:Loading Queries...\n",
      "INFO:financerag.common.loader:Loaded 216 Queries.\n",
      "INFO:financerag.common.loader:Query Example: {'id': 'q00001', 'text': 'What are the service and product offerings from Microsoft'}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize FinDER Task\n",
    "# --------------------------\n",
    "# In this baseline example, we are using the FinDER task, one of the seven available tasks in this project.\n",
    "# If you want to use a different task, for example, 'OtherTask', you can change the task initialization as follows:\n",
    "#\n",
    "# Example:\n",
    "# from financerag.tasks import OtherTask\n",
    "# finder_task = OtherTask()\n",
    "#\n",
    "# For this baseline, we proceed with FinDER.\n",
    "finder_task = FinDER()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T14:51:23.961016Z",
     "iopub.status.busy": "2024-10-04T14:51:23.960728Z",
     "iopub.status.idle": "2024-10-04T14:51:39.063618Z",
     "shell.execute_reply": "2024-10-04T14:51:39.062302Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: nvidia/NV-Embed-v2\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize DenseRetriever model\n",
    "# -------------------------------------\n",
    "# Initialize the retrieval model using SentenceTransformers. This model will be responsible\n",
    "# for encoding both the queries and documents into embeddings.\n",
    "#\n",
    "# You can replace 'intfloat/e5-large-v2' with any other model supported by SentenceTransformers.\n",
    "# For example: 'BAAI/bge-large-en-v1.5', 'Linq-AI-Research/Linq-Embed-Mistral', etc.\n",
    "encoder_model = SentenceTransformerEncoder(\n",
    "    model_name_or_path='nvidia/NV-Embed-v2',\n",
    "    query_prompt='query: ',\n",
    "    doc_prompt='passage: ',\n",
    "    device='cpu',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "retrieval_model = DenseRetrieval(\n",
    "    model=encoder_model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T14:51:39.068549Z",
     "iopub.status.busy": "2024-10-04T14:51:39.068124Z",
     "iopub.status.idle": "2024-10-04T14:54:07.488228Z",
     "shell.execute_reply": "2024-10-04T14:54:07.486678Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Encoding queries...\n",
      "Batches:   0%|          | 0/4 [00:00<?, ?it/s]/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "Batches: 100%|██████████| 4/4 [02:22<00:00, 35.52s/it]\n",
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n",
      "Batches:   0%|          | 0/217 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 4: Perform retrieval\n",
    "# ---------------------\n",
    "# Use the model to retrieve relevant documents for given queries.\n",
    "\n",
    "retrieval_result = finder_task.retrieve(\n",
    "    retriever=retrieval_model\n",
    ")\n",
    "\n",
    "# Print a portion of the retrieval results to verify the output.\n",
    "print(f\"Retrieved results for {len(retrieval_result)} queries. Here's an example of the top 5 documents for the first query:\")\n",
    "\n",
    "for q_id, result in retrieval_result.items():\n",
    "    print(f\"\\nQuery ID: {q_id}\")\n",
    "    # Sort the result to print the top 5 document ID and its score\n",
    "    sorted_results = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for i, (doc_id, score) in enumerate(sorted_results[:5]):\n",
    "        print(f\"  Document {i + 1}: Document ID = {doc_id}, Score = {score}\")\n",
    "\n",
    "    break  # Only show the first query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T14:54:07.495914Z",
     "iopub.status.busy": "2024-10-04T14:54:07.494072Z",
     "iopub.status.idle": "2024-10-04T14:54:09.186831Z",
     "shell.execute_reply": "2024-10-04T14:54:09.185722Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.mllama.configuration_mllama because of the following error (look up to see its traceback):\nNo module named 'transformers.models.mllama.configuration_mllama'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:1778\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/usr/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.mllama.configuration_mllama'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 5: Initialize CrossEncoder Reranker\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# --------------------------------------\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The CrossEncoder model will be used to rerank the retrieved documents based on relevance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     model=CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', device=device),\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFlagEmbedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FlagLLMReranker\n\u001b[0;32m---> 12\u001b[0m reranker \u001b[38;5;241m=\u001b[39m \u001b[43mFlagLLMReranker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBAAI/bge-reranker-v2-gemma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/FlagEmbedding/inference/reranker/decoder_only/base.py:244\u001b[0m, in \u001b[0;36mBaseLLMReranker.__init__\u001b[0;34m(self, model_name_or_path, peft_path, use_fp16, use_bf16, query_instruction_for_rerank, query_instruction_format, passage_instruction_for_rerank, passage_instruction_format, cache_dir, trust_remote_code, devices, prompt, batch_size, query_max_length, max_length, normalize, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt \u001b[38;5;241m=\u001b[39m prompt\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    239\u001b[0m     model_name_or_path,\n\u001b[1;32m    240\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    241\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code\n\u001b[1;32m    242\u001b[0m )\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_bf16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_path:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, peft_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:543\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map\n\u001b[1;32m    541\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    542\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m resolve_trust_remote_code(\n\u001b[0;32m--> 543\u001b[0m     trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n\u001b[1;32m    544\u001b[0m )\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# Set the adapter kwargs\u001b[39;00m\n\u001b[1;32m    547\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m adapter_kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:781\u001b[0m, in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitems\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    777\u001b[0m     mapping_items \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    778\u001b[0m         (\n\u001b[1;32m    779\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping[key]),\n\u001b[1;32m    780\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[key]),\n\u001b[0;32m--> 781\u001b[0m         )\n\u001b[1;32m    782\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    784\u001b[0m     ]\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_items \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:777\u001b[0m, in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitems\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 777\u001b[0m     mapping_items \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    778\u001b[0m         (\n\u001b[1;32m    779\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping[key]),\n\u001b[1;32m    780\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[key]),\n\u001b[1;32m    781\u001b[0m         )\n\u001b[1;32m    782\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    784\u001b[0m     ]\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_items \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:693\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:1766\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:1780\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.mllama.configuration_mllama because of the following error (look up to see its traceback):\nNo module named 'transformers.models.mllama.configuration_mllama'"
     ]
    }
   ],
   "source": [
    "# Step 5: Initialize CrossEncoder Reranker\n",
    "# --------------------------------------\n",
    "# The CrossEncoder model will be used to rerank the retrieved documents based on relevance.\n",
    "#\n",
    "# You can replace 'cross-encoder/ms-marco-MiniLM-L-12-v2' with any other model supported by CrossEncoder.\n",
    "# For example: 'cross-encoder/ms-marco-TinyBERT-L-2', 'cross-encoder/stsb-roberta-large', etc.\n",
    "reranker = CrossEncoderReranker(\n",
    "    model=CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', device=device),\n",
    ")\n",
    "\n",
    "from FlagEmbedding import FlagLLMReranker\n",
    "reranker = CrossEncoderReranker(model=FlagLLMReranker('BAAI/bge-reranker-v2-gemma'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T14:54:09.190909Z",
     "iopub.status.busy": "2024-10-04T14:54:09.190659Z",
     "iopub.status.idle": "2024-10-04T14:54:54.978852Z",
     "shell.execute_reply": "2024-10-04T14:54:54.977781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-500....\n",
      "Batches:   0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 85/85 [00:36<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking results for 216 queries. Here's an example of the top 5 documents for the first query:\n",
      "\n",
      "Query ID: q00001\n",
      "  Document 1: Document ID = MSFT20230254, Score = 6.622740745544434\n",
      "  Document 2: Document ID = MSFT20230233, Score = 6.088944435119629\n",
      "  Document 3: Document ID = MSFT20230230, Score = 5.898367404937744\n",
      "  Document 4: Document ID = MSFT20230236, Score = 5.747088432312012\n",
      "  Document 5: Document ID = MSFT20230216, Score = 5.488572120666504\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Perform reranking\n",
    "# -------------------------\n",
    "# Rerank the top 100 retrieved documents using the CrossEncoder model.\n",
    "reranking_result = finder_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=retrieval_result,\n",
    "    top_k=500,  # Rerank the top 100 documents\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "# Print a portion of the reranking results to verify the output.\n",
    "print(f\"Reranking results for {len(reranking_result)} queries. Here's an example of the top 5 documents for the first query:\")\n",
    "\n",
    "for q_id, result in reranking_result.items():\n",
    "    print(f\"\\nQuery ID: {q_id}\")\n",
    "    # Sort the result to print the top 5 document ID and its score\n",
    "    sorted_results = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for i, (doc_id, score) in enumerate(sorted_results[:5]):\n",
    "        print(f\"  Document {i + 1}: Document ID = {doc_id}, Score = {score}\")\n",
    "\n",
    "    break  # Only show the first query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T14:54:54.989320Z",
     "iopub.status.busy": "2024-10-04T14:54:54.989100Z",
     "iopub.status.idle": "2024-10-04T14:54:55.005455Z",
     "shell.execute_reply": "2024-10-04T14:54:55.004477Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.tasks.BaseTask:Output directory set to: ./results/FinDER\n",
      "INFO:financerag.tasks.BaseTask:Saving top 10 results to CSV file: ./results/FinDER/results.csv\n",
      "INFO:financerag.tasks.BaseTask:Writing header ['query_id', 'corpus_id'] to CSV.\n",
      "INFO:financerag.tasks.BaseTask:Top 10 results saved successfully to ./results/FinDER/results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to ./results/FinDER/results.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Save results\n",
    "# -------------------\n",
    "# Save the results to the specified output directory as a CSV file.\n",
    "output_dir = './results'\n",
    "finder_task.save_results(output_dir=output_dir)\n",
    "\n",
    "# Confirm the results have been saved.\n",
    "print(f\"Results have been saved to {output_dir}/FinDER/results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9677683,
     "sourceId": 85594,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
